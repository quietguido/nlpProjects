
1. Familiarize yourself with the Yelp data. Split the provided train set into (new) train (70% of the whole dataset) and validation (10% of the whole dataset) subsets. Describe the data. (5) Note: We will reuse the splits in the next labs, so fix the random state.
2. The starter notebook uses CountVectorizer to convert texts into vectors and MultinomialNB classifier. Experiment with different vectorizer parameters (use of stopwords, minimum document frequency, binary features, and lowercasing) and the smoothing parameter of the NB classifier. Select the best configuration based on the validation set, apply it to the test set. (25)
a. Experiment with a different classifier (e.g. SVM). (10)

param_grid = {
    'vectorizer__min_df': [0.01, 0.1, 1],
    'vectorizer__stop_words': [None, 'english'],
    'vectorizer__binary': [False, True],
    'vectorizer__lowercase': [True, False],
    'clf__alpha': [0.01, 0.1, 0.5, 1.0]
}
Different combinations of the vectorizeer and alpha smothing parameters where used 
Over all: 
 3 * 2 * 2 * 2 * 4 = 96 pairs 

MultinomialNB
Best combination first model: {
    'model': 'first_model',
    'min_df': 1,
    'stop_words': None,
    'binary': False,
    'lowercase': True,
    'alpha': 0.5
    }
Best accuracy first model: 0.85
Support Vector Classification.
Best combination second model: {
    'model': 'second_model', 
    'min_df': 1, 
    'stop_words': 'english', 
    'binary': False, 
    'lowercase': False, 
    'alpha': 0.01
    }
Best accuracy second model: 0.84

In summary, the MNB model with its specific hyperparameters performed slightly better 
than the SVC model with its specific hyperparameters on the validation set.
However, the performance of these models on unseen test data may vary.
We can observe that the best combination of hyperparameters for the MNB model is 
min_df=1, stop_words=None, binary=False, lowercase=True, and alpha=0.5,
while the best combination of hyperparameters for the SVC model is
min_df=1, stop_words='english', binary=False, lowercase=False, and alpha=0.01.
the minimum document frequency meaning a word must be found 
in at least one document to be included in the vocabulary,
showed best results when set to 1 in both models

Also, the same corrolation can be seen in binary parameter = false, which means that 
the count of words in each document is used as the feature value.
However, the lowercase, stop_words and alpha parameters showed different results in both models.
Overall, distribution suggested that the MNB model performed better than the SVC model.
But, huge factor of the performance of both fectors were connected to 
how we split the data using randmo_state. 






3. Familiarize yourself with the SentiWord lexicon. Process and describe the data. (5)
4. Develop a lexicon-based sentiment classifier using Stanza for lemmatization and POS-
tagging. (Mind difference in labeling: sentences: 0 – negative, 1 – positive; words: continuous scores from the range [-1, 1]. Note that SentiWords and Stanza use different POS tag sets.) (35)
a. Use validation set to optimize the threshold value for binary classification. (10) 5. Summarize and compare the evaluation results (accuracy on the test set) of all tested
configurations. Analyze misclassified examples. (10)
5. Summarize and compare the evaluation results (accuracy on the test set) of all tested
configurations. Analyze misclassified examples. (10)



validation_texts = [
    "This movie was a fantastic journey through imagination.",
    "I've never been more disappointed by a meal.",
    "The sunset was breathtaking and made my day.",
    "It was a terrible experience, and I wouldn't recommend it.",
    "The book was captivating and I couldn't put it down.",
    "The service was slow and unattentive.",
    "What a wonderful performance, truly moving!",
    "This gadget is a total waste of money.",
    "A delightful experience from start to finish.",
    "I've had better days; today was not great."
]

validation_labels = [
    1,  # Positive
    0,  # Negative
    1,  # Positive
    0,  # Negative
    1,  # Positive
    0,  # Negative
    1,  # Positive
    0,  # Negative
    1,  # Positive
    0   # Negative
]


# Assuming validation_texts and validation_labels are defined
# validation_texts is a list of texts in the validation set
# validation_labels is a list of corresponding 
# true labels (0 for negative, 1 for positive)


# Best Threshold: 0.0, Best Accuracy: 0.7
The task involved building a sentiment classifier using pre-existing 
sentiment scores from the SentiWord lexicon and leveraging Stanza 
for text preprocessing. By fine-tuning the classification threshold using 
a validation set, the best threshold of 0.0 yielded a 70% accuracy rate.
Despite its simplicity and transparency, this approach is contingent upon
the accuracy of the sentiment lexicon and text processing steps, potentially
limiting its effectiveness in capturing nuanced sentiment nuances. 
Further refinement and evaluation across varied datasets are essential 
for enhancing its reliability and applicability in real-world scenarios.

Example usage: 

# [('I', 'PRON'), ('have', 'AUX'), ('have', 'VERB'), ('good', 'ADJ'), ('day', 'NOUN'), (';', 'PUNCT'), ('today', 'NOUN'), ('be', 'AUX'), ('not', 'PART'), ('great', 'ADJ'), ('.', 'PUNCT')]
# I .
# have .
# have v
# adding score 0.22859
# good .
# day n
# adding score 0.34355
# ; .
# today n
# adding score 0.10554
# be .
# not .
# great .
# . .

 Also from the usage case you can observe that the lemmatization and POS tagging
 was done correctly and the sentiment score was calculated correctly. 

The approach utilized the SentiWord lexicon for obtaining 
continuous sentiment scores for words within the -1 to 1 range, 
and these scores were combined with linguistic elements extracted through Stanza, 
encompassing lemmatization and pos.A critical aspect to 
consider was the differing POS tag sets used by SentiWords and Stanza, 
which necessitated precise coordination during the classifier's creation.

Reviewing the classifier's output, particularly with regard to incorrectly
 classified cases, provides insightful observations. The examples demonstrate 
 that the classifier was proficient in detecting and compiling sentiment scores
  for words found in the SentiWord lexicon. However, it struggled in situations 
  where the lexicon did not include certain words or when there was a discrepancy 
  in the POS tags between the lexicon and Stanza's output. Notably, pronouns and
   some adjectives, like "great," were unable to influence the sentiment score due
    to their absence in the lexicon or tag mismatches.

Additionally, the decision to use a threshold of 0.0 for
 classification meant that any sentiment score that was 
 zero or positive was classified as positive. This critical choice 
 in setting the threshold emphasizes the classifier's reliance on the 
 sentiment score distribution within the SentiWord lexicon and its capacity 
 to detect nuanced sentiments.